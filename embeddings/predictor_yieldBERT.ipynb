{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31757f6beb15175d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T14:11:46.795552710Z",
     "start_time": "2023-12-12T14:11:46.778476670Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This extension has only been tested with simpletransformers==0.34.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from rxnfp.models import SmilesClassificationModel, SmilesLanguageModelingModel\n",
    "import pandas as pd\n",
    "from rxn_yields.data import generate_buchwald_hartwig_rxns\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import DataStructs, AllChem\n",
    "import numpy as np\n",
    "\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T14:11:58.189384177Z",
     "start_time": "2023-12-12T14:11:50.657881163Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clc1ccccn1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2...</td>\n",
       "      <td>1.387974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brc1ccccn1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2...</td>\n",
       "      <td>-0.796876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCc1ccc(I)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccc...</td>\n",
       "      <td>-0.827835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FC(F)(F)c1ccc(Cl)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd...</td>\n",
       "      <td>-0.464841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COc1ccc(Cl)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2cc...</td>\n",
       "      <td>-1.186082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    labels\n",
       "0  Clc1ccccn1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2...  1.387974\n",
       "1  Brc1ccccn1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2... -0.796876\n",
       "2  CCc1ccc(I)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccc... -0.827835\n",
       "3  FC(F)(F)c1ccc(Cl)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd... -0.464841\n",
       "4  COc1ccc(Cl)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2cc... -1.186082"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('../Модели/yieldBERT/rxn_yields/data/Buchwald-Hartwig/Dreher_and_Doyle_input_data.xlsx', sheet_name='FullCV_01')\n",
    "df['rxn'] = generate_buchwald_hartwig_rxns(df)\n",
    "\n",
    "train_df = df.iloc[:2767][['rxn', 'Output']] \n",
    "test_df = df.iloc[2767:][['rxn', 'Output']] #\n",
    "\n",
    "train_df.columns = ['text', 'labels']\n",
    "test_df.columns = ['text', 'labels']\n",
    "mean = train_df.labels.mean()\n",
    "std = train_df.labels.std()\n",
    "train_df['labels'] = (train_df['labels'] - mean) / std\n",
    "test_df['labels'] = (test_df['labels'] - mean) / std\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e916d5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ligand</th>\n",
       "      <th>Additive</th>\n",
       "      <th>Base</th>\n",
       "      <th>Aryl halide</th>\n",
       "      <th>Output</th>\n",
       "      <th>rxn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC(C)C(C=C(C(C)C)C=C1C(C)C)=C1C2=C(P([C@@]3(C[...</td>\n",
       "      <td>CC1=CC(C)=NO1</td>\n",
       "      <td>CN(C)P(N(C)C)(N(C)C)=NP(N(C)C)(N(C)C)=NCC</td>\n",
       "      <td>ClC1=NC=CC=C1</td>\n",
       "      <td>70.410458</td>\n",
       "      <td>Clc1ccccn1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC(C)C(C=C(C(C)C)C=C1C(C)C)=C1C2=C(P([C@@]3(C[...</td>\n",
       "      <td>O=C(OC)C1=CC=NO1</td>\n",
       "      <td>CN(C)P(N(C)C)(N(C)C)=NP(N(C)C)(N(C)C)=NCC</td>\n",
       "      <td>BrC1=NC=CC=C1</td>\n",
       "      <td>11.064457</td>\n",
       "      <td>Brc1ccccn1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Ligand          Additive  \\\n",
       "0  CC(C)C(C=C(C(C)C)C=C1C(C)C)=C1C2=C(P([C@@]3(C[...     CC1=CC(C)=NO1   \n",
       "1  CC(C)C(C=C(C(C)C)C=C1C(C)C)=C1C2=C(P([C@@]3(C[...  O=C(OC)C1=CC=NO1   \n",
       "\n",
       "                                        Base    Aryl halide     Output  \\\n",
       "0  CN(C)P(N(C)C)(N(C)C)=NP(N(C)C)(N(C)C)=NCC  ClC1=NC=CC=C1  70.410458   \n",
       "1  CN(C)P(N(C)C)(N(C)C)=NP(N(C)C)(N(C)C)=NCC  BrC1=NC=CC=C1  11.064457   \n",
       "\n",
       "                                                 rxn  \n",
       "0  Clc1ccccn1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2...  \n",
       "1  Brc1ccccn1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../Модели/yieldBERT/rxn_yields/trained_models/buchwald_hartwig/FullCV_01_split_2768/checkpoint-1730-epoch-10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "292750cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n",
      "Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"
     ]
    }
   ],
   "source": [
    "trained_yield_bert = SmilesClassificationModel('bert', model_path,\n",
    "                                               num_labels=1,\n",
    "                                               args={\"regression\": True,\n",
    "                                                    'config': {\"output_hidden_states\": True}},\n",
    "                                               use_cuda=False,\n",
    "                                              )\n",
    "                                               #use_cuda=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33753ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "652dece726ae613f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T14:14:06.990008560Z",
     "start_time": "2023-12-12T14:13:58.130113151Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n",
      "Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6421718ecd4ec8aa7b5f2b939ee837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd584fa7ff334269aaad3de4a134ee20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted 46.3 | 38.1 true yield\n",
      "\n",
      "predicted 12.1 | 14.8 true yield\n",
      "\n",
      "predicted 9.0 | 12.2 true yield\n",
      "\n",
      "predicted 11.6 | 8.3 true yield\n",
      "\n",
      "predicted 1.9 | 1.1 true yield\n",
      "\n",
      "predicted 46.0 | 44.4 true yield\n",
      "\n",
      "predicted 48.8 | 53.4 true yield\n",
      "\n",
      "predicted 1.6 | 1.7 true yield\n",
      "\n",
      "predicted 14.9 | 10.2 true yield\n",
      "\n",
      "predicted -0.3 | 0.0 true yield\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yield_predicted = trained_yield_bert.predict(test_df.head(10).text.values)[0]\n",
    "yield_predicted = yield_predicted * std + mean\n",
    "\n",
    "yield_true = test_df.head(10).labels.values\n",
    "yield_true = yield_true * std + mean\n",
    "\n",
    "for rxn, pred, true in zip(test_df.head(10).text.values, yield_predicted, yield_true):\n",
    "    # print(rxn)\n",
    "    print(f\"predicted {pred:.1f} | {true:.1f} true yield\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9ba6863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Brc1ccccn1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.COc1ccc(OC)c(P(C(C)(C)C)C(C)(C)C)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C.CN(C)C(=NC(C)(C)C)N(C)C.c1ccc(CN(Cc2ccccc2)c2ccno2)cc1>>Cc1ccc(Nc2ccccn2)cc1',\n",
       "       'Brc1cccnc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.CC(C)c1cc(C(C)C)c(-c2ccccc2P(C(C)(C)C)C(C)(C)C)c(C(C)C)c1.CCN=P(N=P(N(C)C)(N(C)C)N(C)C)(N(C)C)N(C)C.COC(=O)c1ccno1>>Cc1ccc(Nc2cccnc2)cc1',\n",
       "       'FC(F)(F)c1ccc(Br)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.COc1ccc(OC)c(P([C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)[C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C.CCN=P(N=P(N(C)C)(N(C)C)N(C)C)(N(C)C)N(C)C.Fc1cccc(F)c1-c1ccno1>>Cc1ccc(Nc2ccc(C(F)(F)F)cc2)cc1',\n",
       "       'FC(F)(F)c1ccc(Cl)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.CC(C)c1cc(C(C)C)c(-c2ccccc2P(C2CCCCC2)C2CCCCC2)c(C(C)C)c1.CN1CCCN2CCCN=C12.CCOC(=O)c1cc(OC)no1>>Cc1ccc(Nc2ccc(C(F)(F)F)cc2)cc1',\n",
       "       'COc1ccc(Cl)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.COc1ccc(OC)c(P([C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)[C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C.CN1CCCN2CCCN=C12.c1ccc(-c2ccon2)cc1>>COc1ccc(Nc2ccc(C)cc2)cc1',\n",
       "       'COc1ccc(I)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.CC(C)c1cc(C(C)C)c(-c2ccccc2P(C(C)(C)C)C(C)(C)C)c(C(C)C)c1.CN1CCCN2CCCN=C12.COC(=O)c1cc(-c2ccco2)on1>>COc1ccc(Nc2ccc(C)cc2)cc1',\n",
       "       'COc1ccc(I)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.COc1ccc(OC)c(P([C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)[C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C.CN(C)C(=NC(C)(C)C)N(C)C.Cc1ccon1>>COc1ccc(Nc2ccc(C)cc2)cc1',\n",
       "       'COc1ccc(Cl)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.COc1ccc(OC)c(P([C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)[C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C.CCN=P(N=P(N(C)C)(N(C)C)N(C)C)(N(C)C)N(C)C.c1ccc(-c2ccno2)cc1>>COc1ccc(Nc2ccc(C)cc2)cc1',\n",
       "       'FC(F)(F)c1ccc(Br)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.COc1ccc(OC)c(P(C(C)(C)C)C(C)(C)C)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C.CN(C)C(=NC(C)(C)C)N(C)C.COC(=O)c1ccno1>>Cc1ccc(Nc2ccc(C(F)(F)F)cc2)cc1',\n",
       "       'COc1ccc(Cl)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.CC(C)c1cc(C(C)C)c(-c2ccccc2P(C2CCCCC2)C2CCCCC2)c(C(C)C)c1.CCN=P(N=P(N(C)C)(N(C)C)N(C)C)(N(C)C)N(C)C.COC(=O)c1cc(-c2cccs2)on1>>COc1ccc(Nc2ccc(C)cc2)cc1'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(10).text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8d84a6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n",
      "Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b23315f94c4bf69ad8f575455a14f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4ffae76b4f4cc7ac9dbf9d068cb704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reaction: Brc1ccccn1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.COc1ccc(OC)c(P(C(C)(C)C)C(C)(C)C)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C.CN(C)C(=NC(C)(C)C)N(C)C.c1ccc(CN(Cc2ccccc2)c2ccno2)cc1>>Cc1ccc(Nc2ccccn2)cc1\n",
      "Predicted Yield: 0.5017\n",
      "\n",
      "Reaction: Brc1cccnc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.CC(C)c1cc(C(C)C)c(-c2ccccc2P(C(C)(C)C)C(C)(C)C)c(C(C)C)c1.CCN=P(N=P(N(C)C)(N(C)C)N(C)C)(N(C)C)N(C)C.COC(=O)c1ccno1>>Cc1ccc(Nc2cccnc2)cc1\n",
      "Predicted Yield: 0.5017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Создание копии модели\n",
    "frozen_model = SmilesClassificationModel('bert', model_path,\n",
    "                                         num_labels=1,\n",
    "                                         args={\"regression\": True},\n",
    "                                         use_cuda=False)\n",
    "                                         #use_cuda=torch.cuda.is_available())\n",
    "\n",
    "# Заморозка параметров классификатора\n",
    "for param in frozen_model.model.classifier.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in frozen_model.model.bert.pooler.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in frozen_model.model.bert.encoder.layer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in frozen_model.model.bert.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "# Предсказания для тестовых данных\n",
    "predictions = frozen_model.predict(test_df.text.values)\n",
    "\n",
    "# Вывод нескольких строчек\n",
    "for rxn, pred in zip(test_df.text.values[:10], predictions[:10]):\n",
    "    print(f\"Reaction: {rxn}\")\n",
    "    print(f\"Predicted Yield: {pred[0]:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "407e7ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n",
      "Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-bc8f479e2bad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Получение вывода модели\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrozen_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Взятие вектора представления из последнего скрытого слоя\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages/simpletransformers/classification/transformer_models/bert_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         )\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# Complains if input_embeds is kept\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         embedding_output = self.embeddings(\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         )\n\u001b[1;32m    729\u001b[0m         encoder_outputs = self.encoder(\n",
      "\u001b[0;32m~/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "frozen_model = SmilesClassificationModel('bert', model_path,\n",
    "                                         num_labels=1,\n",
    "                                         args={\"regression\": True},\n",
    "                                         use_cuda=False)\n",
    "# Заморозка всех параметров модели\n",
    "for param in frozen_model.model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Установка модели в режим оценки\n",
    "frozen_model.model.eval()\n",
    "\n",
    "# Создание отдельного токенизатора\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "#frozen_model.model = frozen_model.model.to('cpu')\n",
    "# Теперь, чтобы получить эмбеддинги\n",
    "embeddings = []\n",
    "\n",
    "for rxn_text in test_df.text.values[:10]:\n",
    "    # Токенизация текста\n",
    "    inputs = tokenizer.encode_plus(rxn_text,\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   truncation=True,\n",
    "                                   padding=True)\n",
    "    \n",
    "    # Перемещение токенизированных данных на устройство CPU\n",
    "    #inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    # Получение вывода модели\n",
    "    with torch.no_grad():\n",
    "        outputs = frozen_model.model(**inputs)\n",
    "\n",
    "    # Взятие вектора представления из последнего скрытого слоя\n",
    "    last_hidden_states = outputs[0][\"last_hidden_state\"]\n",
    "    embedding = last_hidden_states.mean(dim=1).squeeze().numpy()\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "# Вывод первого эмбеддинга\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7106c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, inputs):\n",
    "    # Токенизируем SMILES-строку\n",
    "    encoded_inputs = model.tokenizer.encode(inputs)\n",
    "\n",
    "    # Преобразуем закодированные входы в словарь\n",
    "    encoded_inputs_dict = {\"input_ids\": torch.tensor(encoded_inputs).unsqueeze(0).to(model.device)}  # Предполагаем, что первый элемент списка - это input_ids\n",
    "\n",
    "    # Подаем словарь в модель\n",
    "    outputs = model.model(**encoded_inputs_dict)\n",
    "\n",
    "    # Извлекаем эмбеддинги из выходного слоя\n",
    "    outputs = model.model(**encoded_inputs_dict)\n",
    "    embeddings = outputs[0]  # Доступ к первому элементу кортежа\n",
    "\n",
    "    # Возвращаем векторы эмбеддингов\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b2f78e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, inputs):\n",
    "  \"\"\"\n",
    "  Получает эмбеддинги для списка строк.\n",
    "\n",
    "  Args:\n",
    "    model: Модель обработки текста.\n",
    "    inputs: Список строк.\n",
    "\n",
    "  Returns:\n",
    "    Массив эмбеддингов.\n",
    "  \"\"\"\n",
    "\n",
    "  embeddings = []\n",
    "  for input in inputs:\n",
    "    # Токенизируем строку.\n",
    "    tokenized_input = model.tokenizer.encode(input)\n",
    "\n",
    "    # Преобразуем закодированные входы в словарь\n",
    "    encoded_inputs_dict = {\"input_ids\": torch.tensor(tokenized_input).unsqueeze(0).to(model.device)}\n",
    "\n",
    "    # Подаем словарь в модель\n",
    "    outputs = model.model(**encoded_inputs_dict)\n",
    "\n",
    "    # Извлекаем эмбеддинги из выходного слоя\n",
    "    embeddings.append(outputs[0])\n",
    "\n",
    "  return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "802d400e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-215-fdbbb7724eff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "#model = SmilesClassificationModel.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = test_df.text.values[:10]\n",
    "embeddings = get_embeddings(frozen_model, inputs)\n",
    "\n",
    "for embedding in embeddings:\n",
    "    print(embedding.shape)\n",
    "print(embeddings.shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b4b07f99",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-209-8280eec4b57a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Отсоединяем тензор от графа вычислений\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0membeddings_detached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Преобразуем отсоединенный тензор в массив NumPy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Отсоединяем тензор от графа вычислений\n",
    "embeddings_detached = embeddings.detach()\n",
    "\n",
    "# Преобразуем отсоединенный тензор в массив NumPy\n",
    "embeddings_np = np.array(embeddings_detached)\n",
    "\n",
    "# Выводим числовые значения\n",
    "print(embeddings_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8241c9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00893622]]\n"
     ]
    }
   ],
   "source": [
    "inputs = \"C1=CC=C(C=C1)O\"\n",
    "\n",
    "# Удаляем пробелы из SMILES-строки\n",
    "inputs = inputs.replace(\" \", \"\")\n",
    "\n",
    "# Получаем эмбеддинги\n",
    "embeddings = get_embeddings(frozen_model, inputs)\n",
    "\n",
    "# Отсоединяем тензор от графа вычислений\n",
    "embeddings_detached = embeddings.detach()\n",
    "\n",
    "# Преобразуем отсоединенный тензор в массив NumPy\n",
    "embeddings_np = np.array(embeddings_detached)\n",
    "\n",
    "# Выводим числовые значения\n",
    "print(embeddings_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0c5f279b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0089, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a648fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a5f9dba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n",
      "Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"
     ]
    }
   ],
   "source": [
    "frozen_model = SmilesClassificationModel('bert', model_path,\n",
    "                                         #freeze_encoder=True,\n",
    "                                         use_cuda=False)\n",
    "\n",
    "#model = SmilesClassificationModel(\n",
    "#   model_type=\"bert\",\n",
    "#    model_name=\"bert-base-uncased\",\n",
    "#    model_path=model_path,\n",
    "#    freeze_encoder=True  # или freeze_all_but_one=True\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4740be53",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SmilesClassificationModel' object has no attribute 'forward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-d2745bf48b71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtensor_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0minput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtensor_inputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrozen_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Используем метод forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SmilesClassificationModel' object has no attribute 'forward'"
     ]
    }
   ],
   "source": [
    "text_data = test_df.text.values.astype(str).tolist()\n",
    "all_tokens = []  # Список для хранения всех токенов\n",
    "for text in text_data:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    all_tokens.extend(tokens)  # Добавляем токены в общий список\n",
    "\n",
    "input_ids = tokenizer.convert_tokens_to_ids(all_tokens)  # Теперь это список токенов, а не список списков\n",
    "tensor_inputs = torch.tensor([input_ids])\n",
    "input_dict = {\"input_ids\": tensor_inputs}\n",
    "outputs = frozen_model.forward(**input_dict)  # Используем метод forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77068bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state = outputs.last_hidden_state\n",
    "vector_representations = last_hidden_state[:, 0, :]  # Извлечь представления из первого токена"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6ff7d4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "97b348ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b954cc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer name: bert.embeddings.word_embeddings.weight, Requires gradient: False\n",
      "Layer name: bert.embeddings.position_embeddings.weight, Requires gradient: False\n",
      "Layer name: bert.embeddings.token_type_embeddings.weight, Requires gradient: False\n",
      "Layer name: bert.embeddings.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.embeddings.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.0.attention.self.query.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.0.attention.self.query.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.0.attention.self.key.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.0.attention.self.key.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.0.attention.self.value.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.0.attention.self.value.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.0.attention.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.0.attention.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.0.attention.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.0.attention.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.0.intermediate.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.0.intermediate.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.0.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.0.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.0.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.0.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.1.attention.self.query.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.1.attention.self.query.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.1.attention.self.key.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.1.attention.self.key.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.1.attention.self.value.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.1.attention.self.value.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.1.attention.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.1.attention.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.1.attention.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.1.attention.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.1.intermediate.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.1.intermediate.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.1.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.1.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.1.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.1.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.2.attention.self.query.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.2.attention.self.query.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.2.attention.self.key.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.2.attention.self.key.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.2.attention.self.value.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.2.attention.self.value.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.2.attention.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.2.attention.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.2.attention.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.2.attention.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.2.intermediate.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.2.intermediate.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.2.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.2.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.2.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.2.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.3.attention.self.query.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.3.attention.self.query.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.3.attention.self.key.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.3.attention.self.key.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.3.attention.self.value.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.3.attention.self.value.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.3.attention.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.3.attention.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.3.attention.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.3.attention.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.3.intermediate.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.3.intermediate.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.3.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.3.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.3.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.3.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.4.attention.self.query.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.4.attention.self.query.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.4.attention.self.key.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.4.attention.self.key.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.4.attention.self.value.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.4.attention.self.value.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.4.attention.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.4.attention.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.4.attention.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.4.attention.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.4.intermediate.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.4.intermediate.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.4.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.4.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.4.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.4.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.5.attention.self.query.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.5.attention.self.query.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.5.attention.self.key.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.5.attention.self.key.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.5.attention.self.value.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.5.attention.self.value.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.5.attention.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.5.attention.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.5.attention.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.5.attention.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.5.intermediate.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.5.intermediate.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.5.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.5.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.5.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.5.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.6.attention.self.query.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.6.attention.self.query.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.6.attention.self.key.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.6.attention.self.key.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.6.attention.self.value.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.6.attention.self.value.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.6.attention.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.6.attention.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.6.attention.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.6.attention.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.6.intermediate.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.6.intermediate.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.6.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.6.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.6.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.6.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.7.attention.self.query.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.7.attention.self.query.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.7.attention.self.key.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.7.attention.self.key.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.7.attention.self.value.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.7.attention.self.value.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.7.attention.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.7.attention.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.7.attention.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.7.attention.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.7.intermediate.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.7.intermediate.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.7.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.7.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.7.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.7.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.8.attention.self.query.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.8.attention.self.query.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.8.attention.self.key.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.8.attention.self.key.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.8.attention.self.value.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.8.attention.self.value.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.8.attention.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.8.attention.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.8.attention.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.8.attention.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.8.intermediate.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.8.intermediate.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.8.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.8.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.8.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.8.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.9.attention.self.query.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.9.attention.self.query.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.9.attention.self.key.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.9.attention.self.key.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.9.attention.self.value.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.9.attention.self.value.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.9.attention.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.9.attention.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.9.attention.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.9.attention.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.9.intermediate.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.9.intermediate.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.9.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.9.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.9.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.9.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.10.attention.self.query.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.10.attention.self.query.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.10.attention.self.key.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.10.attention.self.key.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.10.attention.self.value.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.10.attention.self.value.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.10.attention.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.10.attention.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.10.attention.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.10.attention.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.10.intermediate.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.10.intermediate.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.10.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.10.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.10.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.10.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.11.attention.self.query.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.11.attention.self.query.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.11.attention.self.key.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.11.attention.self.key.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.11.attention.self.value.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.11.attention.self.value.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.11.attention.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.11.attention.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.11.attention.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.11.attention.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.11.intermediate.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.11.intermediate.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.11.output.dense.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.11.output.dense.bias, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.11.output.LayerNorm.weight, Requires gradient: False\n",
      "Layer name: bert.encoder.layer.11.output.LayerNorm.bias, Requires gradient: False\n",
      "Layer name: bert.pooler.dense.weight, Requires gradient: False\n",
      "Layer name: bert.pooler.dense.bias, Requires gradient: False\n",
      "Layer name: classifier.weight, Requires gradient: True\n",
      "Layer name: classifier.bias, Requires gradient: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in frozen_model.model.named_parameters():\n",
    "    print(f\"Layer name: {name}, Requires gradient: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "02c31c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1188\n",
      "1188\n",
      "0.5017356\n",
      "0.5017356\n",
      "-0.7569299\n",
      "-0.7569299\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df.text.values))\n",
    "print(len(predictions[1]))\n",
    "print(predictions[0][0])\n",
    "print(predictions[1][0])\n",
    "print(predictions[0][1])\n",
    "print(predictions[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6e9e47c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n",
      "Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Создание копии модели\n",
    "frozen_model = SmilesClassificationModel('bert', model_path,\n",
    "                                         num_labels=1,\n",
    "                                         args={\"regression\": True},\n",
    "                                         use_cuda=torch.cuda.is_available())\n",
    "\n",
    "# Заморозка параметров классификатора\n",
    "for param in frozen_model.model.classifier.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Предсказания для тестовых данных\n",
    "embeddings = []\n",
    "\n",
    "for text in test_df.text.values:\n",
    "    # Получение внутреннего представления из последнего скрытого слоя\n",
    "    with torch.no_grad():\n",
    "        input_ids = frozen_model.tokenizer.encode(text, return_tensors=\"pt\")\n",
    "        outputs = frozen_model.model(input_ids)\n",
    "\n",
    "    # Взятие вектора представления из последнего скрытого слоя\n",
    "    last_hidden_states = outputs[0].mean(dim=1).squeeze().numpy()\n",
    "    embeddings.append(last_hidden_states)\n",
    "\n",
    "# embeddings теперь содержит внутренние представления для каждой реакции\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecc3608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ee2e911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reaction: Brc1ccccn1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.COc1ccc(OC)c(P(C(C)(C)C)C(C)(C)C)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C.CN(C)C(=NC(C)(C)C)N(C)C.c1ccc(CN(Cc2ccccc2)c2ccno2)cc1>>Cc1ccc(Nc2ccccn2)cc1\n",
      "Predicted Yield: 0.5017\n",
      "\n",
      "Reaction: Brc1cccnc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.CC(C)c1cc(C(C)C)c(-c2ccccc2P(C(C)(C)C)C(C)(C)C)c(C(C)C)c1.CCN=P(N=P(N(C)C)(N(C)C)N(C)C)(N(C)C)N(C)C.COC(=O)c1ccno1>>Cc1ccc(Nc2cccnc2)cc1\n",
      "Predicted Yield: 0.5017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rxn, pred in zip(test_df.text.values[:10], predictions[:10]):\n",
    "    print(f\"Reaction: {rxn}\")\n",
    "    print(f\"Predicted Yield: {pred[0].item():.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6748f323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer name: bert.embeddings.word_embeddings.weight, Size: torch.Size([591, 256])\n",
      "Layer name: bert.embeddings.position_embeddings.weight, Size: torch.Size([512, 256])\n",
      "Layer name: bert.embeddings.token_type_embeddings.weight, Size: torch.Size([2, 256])\n",
      "Layer name: bert.embeddings.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.embeddings.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.0.attention.self.query.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.0.attention.self.query.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.0.attention.self.key.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.0.attention.self.key.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.0.attention.self.value.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.0.attention.self.value.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([512, 256])\n",
      "Layer name: bert.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([512])\n",
      "Layer name: bert.encoder.layer.0.output.dense.weight, Size: torch.Size([256, 512])\n",
      "Layer name: bert.encoder.layer.0.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.1.attention.self.query.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.1.attention.self.query.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.1.attention.self.key.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.1.attention.self.key.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.1.attention.self.value.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.1.attention.self.value.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([512, 256])\n",
      "Layer name: bert.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([512])\n",
      "Layer name: bert.encoder.layer.1.output.dense.weight, Size: torch.Size([256, 512])\n",
      "Layer name: bert.encoder.layer.1.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.2.attention.self.query.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.2.attention.self.query.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.2.attention.self.key.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.2.attention.self.key.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.2.attention.self.value.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.2.attention.self.value.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([512, 256])\n",
      "Layer name: bert.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([512])\n",
      "Layer name: bert.encoder.layer.2.output.dense.weight, Size: torch.Size([256, 512])\n",
      "Layer name: bert.encoder.layer.2.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.3.attention.self.query.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.3.attention.self.query.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.3.attention.self.key.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.3.attention.self.key.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.3.attention.self.value.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.3.attention.self.value.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([512, 256])\n",
      "Layer name: bert.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([512])\n",
      "Layer name: bert.encoder.layer.3.output.dense.weight, Size: torch.Size([256, 512])\n",
      "Layer name: bert.encoder.layer.3.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.4.attention.self.query.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.4.attention.self.query.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.4.attention.self.key.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.4.attention.self.key.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.4.attention.self.value.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.4.attention.self.value.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([512, 256])\n",
      "Layer name: bert.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([512])\n",
      "Layer name: bert.encoder.layer.4.output.dense.weight, Size: torch.Size([256, 512])\n",
      "Layer name: bert.encoder.layer.4.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.5.attention.self.query.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.5.attention.self.query.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.5.attention.self.key.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.5.attention.self.key.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.5.attention.self.value.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.5.attention.self.value.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([512, 256])\n",
      "Layer name: bert.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([512])\n",
      "Layer name: bert.encoder.layer.5.output.dense.weight, Size: torch.Size([256, 512])\n",
      "Layer name: bert.encoder.layer.5.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.6.attention.self.query.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.6.attention.self.query.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.6.attention.self.key.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.6.attention.self.key.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.6.attention.self.value.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.6.attention.self.value.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([512, 256])\n",
      "Layer name: bert.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([512])\n",
      "Layer name: bert.encoder.layer.6.output.dense.weight, Size: torch.Size([256, 512])\n",
      "Layer name: bert.encoder.layer.6.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.7.attention.self.query.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.7.attention.self.query.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.7.attention.self.key.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.7.attention.self.key.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.7.attention.self.value.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.7.attention.self.value.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([512, 256])\n",
      "Layer name: bert.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([512])\n",
      "Layer name: bert.encoder.layer.7.output.dense.weight, Size: torch.Size([256, 512])\n",
      "Layer name: bert.encoder.layer.7.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.8.attention.self.query.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.8.attention.self.query.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.8.attention.self.key.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.8.attention.self.key.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.8.attention.self.value.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.8.attention.self.value.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([512, 256])\n",
      "Layer name: bert.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([512])\n",
      "Layer name: bert.encoder.layer.8.output.dense.weight, Size: torch.Size([256, 512])\n",
      "Layer name: bert.encoder.layer.8.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.9.attention.self.query.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.9.attention.self.query.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.9.attention.self.key.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.9.attention.self.key.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.9.attention.self.value.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.9.attention.self.value.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([512, 256])\n",
      "Layer name: bert.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([512])\n",
      "Layer name: bert.encoder.layer.9.output.dense.weight, Size: torch.Size([256, 512])\n",
      "Layer name: bert.encoder.layer.9.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.10.attention.self.query.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.10.attention.self.query.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.10.attention.self.key.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.10.attention.self.key.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.10.attention.self.value.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.10.attention.self.value.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([512, 256])\n",
      "Layer name: bert.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([512])\n",
      "Layer name: bert.encoder.layer.10.output.dense.weight, Size: torch.Size([256, 512])\n",
      "Layer name: bert.encoder.layer.10.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.11.attention.self.query.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.11.attention.self.query.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.11.attention.self.key.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.11.attention.self.key.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.11.attention.self.value.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.11.attention.self.value.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([512, 256])\n",
      "Layer name: bert.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([512])\n",
      "Layer name: bert.encoder.layer.11.output.dense.weight, Size: torch.Size([256, 512])\n",
      "Layer name: bert.encoder.layer.11.output.dense.bias, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([256])\n",
      "Layer name: bert.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([256])\n",
      "Layer name: bert.pooler.dense.weight, Size: torch.Size([256, 256])\n",
      "Layer name: bert.pooler.dense.bias, Size: torch.Size([256])\n",
      "Layer name: classifier.weight, Size: torch.Size([1, 256])\n",
      "Layer name: classifier.bias, Size: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in trained_yield_bert.model.named_parameters():\n",
    "    print(f\"Layer name: {name}, Size: {param.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "579cf437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заморозка параметров слоя классификатора\n",
    "for name, param in trained_yield_bert.model.named_parameters():\n",
    "    if 'classifier' in name:  # Проверка, что это параметры классификатора\n",
    "        param.requires_grad = False\n",
    "    if 'pooler' in name:  # Проверка, что это параметры классификатора\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a0c1fc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: bert.embeddings.word_embeddings.weight, Requires gradient: True\n",
      "Parameter: bert.embeddings.position_embeddings.weight, Requires gradient: True\n",
      "Parameter: bert.embeddings.token_type_embeddings.weight, Requires gradient: True\n",
      "Parameter: bert.embeddings.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.embeddings.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.0.attention.self.query.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.0.attention.self.query.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.0.attention.self.key.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.0.attention.self.key.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.0.attention.self.value.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.0.attention.self.value.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.0.attention.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.0.attention.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.0.attention.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.0.attention.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.0.intermediate.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.0.intermediate.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.0.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.0.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.0.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.0.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.1.attention.self.query.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.1.attention.self.query.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.1.attention.self.key.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.1.attention.self.key.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.1.attention.self.value.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.1.attention.self.value.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.1.attention.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.1.attention.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.1.attention.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.1.attention.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.1.intermediate.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.1.intermediate.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.1.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.1.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.1.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.1.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.2.attention.self.query.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.2.attention.self.query.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.2.attention.self.key.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.2.attention.self.key.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.2.attention.self.value.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.2.attention.self.value.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.2.attention.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.2.attention.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.2.attention.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.2.attention.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.2.intermediate.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.2.intermediate.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.2.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.2.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.2.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.2.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.3.attention.self.query.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.3.attention.self.query.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.3.attention.self.key.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.3.attention.self.key.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.3.attention.self.value.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.3.attention.self.value.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.3.attention.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.3.attention.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.3.attention.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.3.attention.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.3.intermediate.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.3.intermediate.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.3.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.3.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.3.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.3.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.4.attention.self.query.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.4.attention.self.query.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.4.attention.self.key.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.4.attention.self.key.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.4.attention.self.value.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.4.attention.self.value.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.4.attention.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.4.attention.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.4.attention.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.4.attention.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.4.intermediate.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.4.intermediate.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.4.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.4.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.4.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.4.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.5.attention.self.query.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.5.attention.self.query.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.5.attention.self.key.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.5.attention.self.key.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.5.attention.self.value.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.5.attention.self.value.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.5.attention.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.5.attention.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.5.attention.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.5.attention.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.5.intermediate.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.5.intermediate.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.5.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.5.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.5.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.5.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.6.attention.self.query.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.6.attention.self.query.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.6.attention.self.key.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.6.attention.self.key.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.6.attention.self.value.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.6.attention.self.value.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.6.attention.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.6.attention.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.6.attention.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.6.attention.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.6.intermediate.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.6.intermediate.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.6.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.6.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.6.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.6.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.7.attention.self.query.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.7.attention.self.query.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.7.attention.self.key.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.7.attention.self.key.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.7.attention.self.value.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.7.attention.self.value.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.7.attention.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.7.attention.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.7.attention.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.7.attention.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.7.intermediate.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.7.intermediate.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.7.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.7.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.7.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.7.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.8.attention.self.query.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.8.attention.self.query.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.8.attention.self.key.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.8.attention.self.key.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.8.attention.self.value.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.8.attention.self.value.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.8.attention.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.8.attention.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.8.attention.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.8.attention.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.8.intermediate.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.8.intermediate.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.8.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.8.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.8.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.8.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.9.attention.self.query.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.9.attention.self.query.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.9.attention.self.key.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.9.attention.self.key.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.9.attention.self.value.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.9.attention.self.value.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.9.attention.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.9.attention.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.9.attention.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.9.attention.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.9.intermediate.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.9.intermediate.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.9.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.9.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.9.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.9.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.10.attention.self.query.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.10.attention.self.query.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.10.attention.self.key.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.10.attention.self.key.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.10.attention.self.value.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.10.attention.self.value.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.10.attention.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.10.attention.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.10.attention.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.10.attention.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.10.intermediate.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.10.intermediate.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.10.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.10.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.10.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.10.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.11.attention.self.query.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.11.attention.self.query.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.11.attention.self.key.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.11.attention.self.key.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.11.attention.self.value.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.11.attention.self.value.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.11.attention.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.11.attention.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.11.attention.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.11.attention.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.11.intermediate.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.11.intermediate.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.11.output.dense.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.11.output.dense.bias, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.11.output.LayerNorm.weight, Requires gradient: True\n",
      "Parameter: bert.encoder.layer.11.output.LayerNorm.bias, Requires gradient: True\n",
      "Parameter: bert.pooler.dense.weight, Requires gradient: False\n",
      "Parameter: bert.pooler.dense.bias, Requires gradient: False\n",
      "Parameter: classifier.weight, Requires gradient: False\n",
      "Parameter: classifier.bias, Requires gradient: False\n"
     ]
    }
   ],
   "source": [
    "for name, param in trained_yield_bert.model.named_parameters():\n",
    "    print(f'Parameter: {name}, Requires gradient: {param.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "560ead4b037639ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rxn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0aaafe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_yield_bert.predict(smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614891f5",
   "metadata": {},
   "source": [
    "### Далее попробуем достать предикты в векторном представлении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5066b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_rdkit_mol(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    return mol\n",
    "\n",
    "# Получение векторного представления из молекулярного объекта RDKit\n",
    "def mol_to_vector(mol):\n",
    "    # Дескрипторы молекулы\n",
    "    descriptors = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)\n",
    "    # Преобразование дескрипторов в numpy массив\n",
    "    array = np.zeros((1,))\n",
    "    DataStructs.ConvertToNumpyArray(descriptors, array)\n",
    "    \n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7017762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование реакции в векторное представление\n",
    "rxns = test_df.head(10).text.values\n",
    "vectors = []\n",
    "\n",
    "for rxn in rxns:\n",
    "    # Разбивка реакции на отдельные молекулы\n",
    "    reactants, _, products = rxn.split('>')\n",
    "    reactants = reactants.split('.')\n",
    "    products = products.split('.')\n",
    "    \n",
    "    # Преобразование каждой молекулы в вектор и добавление в список\n",
    "    reactant_vectors = [mol_to_vector(smiles_to_rdkit_mol(reactant)) for reactant in reactants]\n",
    "    product_vectors = [mol_to_vector(smiles_to_rdkit_mol(product)) for product in products]\n",
    "    \n",
    "    vectors.extend(reactant_vectors + product_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78ca4760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brc1ccccn1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.COc1ccc(OC)c(P(C(C)(C)C)C(C)(C)C)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C.CN(C)C(=NC(C)(C)C)N(C)C.c1ccc(CN(Cc2ccccc2)c2ccno2)cc1>>Cc1ccc(Nc2ccccn2)cc1\n",
      "Векторное представление: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "Brc1cccnc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.CC(C)c1cc(C(C)C)c(-c2ccccc2P(C(C)(C)C)C(C)(C)C)c(C(C)C)c1.CCN=P(N=P(N(C)C)(N(C)C)N(C)C)(N(C)C)N(C)C.COC(=O)c1ccno1>>Cc1ccc(Nc2cccnc2)cc1\n",
      "Векторное представление: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "FC(F)(F)c1ccc(Br)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.COc1ccc(OC)c(P([C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)[C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C.CCN=P(N=P(N(C)C)(N(C)C)N(C)C)(N(C)C)N(C)C.Fc1cccc(F)c1-c1ccno1>>Cc1ccc(Nc2ccc(C(F)(F)F)cc2)cc1\n",
      "Векторное представление: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "FC(F)(F)c1ccc(Cl)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.CC(C)c1cc(C(C)C)c(-c2ccccc2P(C2CCCCC2)C2CCCCC2)c(C(C)C)c1.CN1CCCN2CCCN=C12.CCOC(=O)c1cc(OC)no1>>Cc1ccc(Nc2ccc(C(F)(F)F)cc2)cc1\n",
      "Векторное представление: [0. 1. 0. ... 0. 0. 0.]\n",
      "\n",
      "COc1ccc(Cl)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.COc1ccc(OC)c(P([C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)[C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C.CN1CCCN2CCCN=C12.c1ccc(-c2ccon2)cc1>>COc1ccc(Nc2ccc(C)cc2)cc1\n",
      "Векторное представление: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "COc1ccc(I)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.CC(C)c1cc(C(C)C)c(-c2ccccc2P(C(C)(C)C)C(C)(C)C)c(C(C)C)c1.CN1CCCN2CCCN=C12.COC(=O)c1cc(-c2ccco2)on1>>COc1ccc(Nc2ccc(C)cc2)cc1\n",
      "Векторное представление: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "COc1ccc(I)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.COc1ccc(OC)c(P([C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)[C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C.CN(C)C(=NC(C)(C)C)N(C)C.Cc1ccon1>>COc1ccc(Nc2ccc(C)cc2)cc1\n",
      "Векторное представление: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "COc1ccc(Cl)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.COc1ccc(OC)c(P([C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)[C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C.CCN=P(N=P(N(C)C)(N(C)C)N(C)C)(N(C)C)N(C)C.c1ccc(-c2ccno2)cc1>>COc1ccc(Nc2ccc(C)cc2)cc1\n",
      "Векторное представление: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "FC(F)(F)c1ccc(Br)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.COc1ccc(OC)c(P(C(C)(C)C)C(C)(C)C)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C.CN(C)C(=NC(C)(C)C)N(C)C.COC(=O)c1ccno1>>Cc1ccc(Nc2ccc(C(F)(F)F)cc2)cc1\n",
      "Векторное представление: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "COc1ccc(Cl)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.CC(C)c1cc(C(C)C)c(-c2ccccc2P(C2CCCCC2)C2CCCCC2)c(C(C)C)c1.CCN=P(N=P(N(C)C)(N(C)C)N(C)C)(N(C)C)N(C)C.COC(=O)c1cc(-c2cccs2)on1>>COc1ccc(Nc2ccc(C)cc2)cc1\n",
      "Векторное представление: [0. 0. 0. ... 0. 0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rxn, vector in zip(test_df.head(10).text.values, vectors):\n",
    "    print(rxn)\n",
    "    print(f'Векторное представление: {vector}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30289be4",
   "metadata": {},
   "source": [
    "**На примере последнего вектора постараемся оценить результат**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7380d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Получился вектор длиной 2048\n",
      "Сумма значений 34.0\n",
      "Содержит 2 уникальных значения, а именно 0.0 и 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Получился вектор длиной {len(vector)}')\n",
    "print(f'Сумма значений {vector.sum()}')\n",
    "\n",
    "unique_values = np.unique(vector)\n",
    "num_unique_values = len(unique_values)\n",
    "print(f'Содержит {num_unique_values} уникальных значения, а именно {unique_values[0]} и {unique_values[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b43c754",
   "metadata": {},
   "source": [
    "**Следовательно, вытащенный вектор состоит из 34 единиц и 2014 нулей**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087c39bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45838273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac4dfd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fdb103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb80f410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23908f01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b1644a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844127b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3644bfd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c6463b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n",
      "Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BertForSequenceClassification object argument after ** must be a mapping, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-d441de81f863>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Инференс\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoded_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Предполагается использование среднего пулинга эмбеддингов\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: BertForSequenceClassification object argument after ** must be a mapping, not Tensor"
     ]
    }
   ],
   "source": [
    "#from simpletransformers.classification import ClassificationArgs, SmilesClassificationModel\n",
    "\n",
    "model = SmilesClassificationModel('bert', model_path,\n",
    "                                         num_labels=1,\n",
    "                                         args={\"regression\": True},\n",
    "                                         use_cuda=False)\n",
    "\n",
    "# Пример строки SMILES\n",
    "smiles_inputs = [\"CCO\", \"CCN\"]\n",
    "\n",
    "# Токенизация и кодирование строк SMILES\n",
    "encoded_inputs = model.tokenizer.encode(smiles_inputs, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Инференс\n",
    "outputs = model.model(**encoded_inputs.data)\n",
    "embeddings = outputs.last_hidden_state.mean(dim=1)  # Предполагается использование среднего пулинга эмбеддингов\n",
    "\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79008589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12, 11, 11, 13]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e928a3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'your_key': array([[12, 11, 11, 13]])}\n"
     ]
    }
   ],
   "source": [
    "numpy_data = encoded_inputs.numpy()\n",
    "\n",
    "# Создаем словарь\n",
    "dict_data = {'your_key': numpy_data}\n",
    "\n",
    "print(dict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2634a850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'your_key': array([[12, 11, 11, 13]])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b801f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871a23c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daddc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "814c124e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'BertTokenizer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-be00d9b89568>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Получение выходов (эмбеддингов) для входных данных\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'BertTokenizer' object is not callable"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "import torch\n",
    "\n",
    "# Загрузка модели\n",
    "model = ClassificationModel(\n",
    "    \"bert\",\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2,\n",
    "    args={\"num_train_epochs\": 1},\n",
    ")\n",
    "\n",
    "# Пример входных данных\n",
    "text = [\"Some text here.\", \"Another text example.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a1dce5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0153,  0.4635]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Получение выходов (эмбеддингов) для входных данных\n",
    "outputs = model.model(model.tokenizer.encode(text, return_tensors=\"pt\"))\n",
    "embeddings = outputs[0]\n",
    "\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c9065d2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-44-ced6170a3842>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-44-ced6170a3842>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    **outputs\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "**outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bb085f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n",
      "Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c76746caa74c90989b406ca942fcb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267db8a4c2d448818e01075713dbbdd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brc1ccccn1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.COc1ccc(OC)c(P(C(C)(C)C)C(C)(C)C)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C.CN(C)C(=NC(C)(C)C)N(C)C.c1ccc(CN(Cc2ccccc2)c2ccno2)cc1>>Cc1ccc(Nc2ccccn2)cc1\n",
      "predicted 0.5 | 0.2 true yield\n",
      "\n",
      "Brc1cccnc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.CC(C)c1cc(C(C)C)c(-c2ccccc2P(C(C)(C)C)C(C)(C)C)c(C(C)C)c1.CCN=P(N=P(N(C)C)(N(C)C)N(C)C)(N(C)C)N(C)C.COC(=O)c1ccno1>>Cc1ccc(Nc2cccnc2)cc1\n",
      "predicted -0.8 | -0.7 true yield\n",
      "\n",
      "FC(F)(F)c1ccc(Br)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.COc1ccc(OC)c(P([C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)[C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C.CCN=P(N=P(N(C)C)(N(C)C)N(C)C)(N(C)C)N(C)C.Fc1cccc(F)c1-c1ccno1>>Cc1ccc(Nc2ccc(C(F)(F)F)cc2)cc1\n",
      "predicted -0.9 | -0.8 true yield\n",
      "\n",
      "FC(F)(F)c1ccc(Cl)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.CC(C)c1cc(C(C)C)c(-c2ccccc2P(C2CCCCC2)C2CCCCC2)c(C(C)C)c1.CN1CCCN2CCCN=C12.CCOC(=O)c1cc(OC)no1>>Cc1ccc(Nc2ccc(C(F)(F)F)cc2)cc1\n",
      "predicted -0.8 | -0.9 true yield\n",
      "\n",
      "COc1ccc(Cl)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.COc1ccc(OC)c(P([C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)[C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C.CN1CCCN2CCCN=C12.c1ccc(-c2ccon2)cc1>>COc1ccc(Nc2ccc(C)cc2)cc1\n",
      "predicted -1.1 | -1.2 true yield\n",
      "\n",
      "COc1ccc(I)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.CC(C)c1cc(C(C)C)c(-c2ccccc2P(C(C)(C)C)C(C)(C)C)c(C(C)C)c1.CN1CCCN2CCCN=C12.COC(=O)c1cc(-c2ccco2)on1>>COc1ccc(Nc2ccc(C)cc2)cc1\n",
      "predicted 0.5 | 0.4 true yield\n",
      "\n",
      "COc1ccc(I)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.COc1ccc(OC)c(P([C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)[C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C.CN(C)C(=NC(C)(C)C)N(C)C.Cc1ccon1>>COc1ccc(Nc2ccc(C)cc2)cc1\n",
      "predicted 0.6 | 0.8 true yield\n",
      "\n",
      "COc1ccc(Cl)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.COc1ccc(OC)c(P([C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)[C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C.CCN=P(N=P(N(C)C)(N(C)C)N(C)C)(N(C)C)N(C)C.c1ccc(-c2ccno2)cc1>>COc1ccc(Nc2ccc(C)cc2)cc1\n",
      "predicted -1.1 | -1.1 true yield\n",
      "\n",
      "FC(F)(F)c1ccc(Br)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.COc1ccc(OC)c(P(C(C)(C)C)C(C)(C)C)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C.CN(C)C(=NC(C)(C)C)N(C)C.COC(=O)c1ccno1>>Cc1ccc(Nc2ccc(C(F)(F)F)cc2)cc1\n",
      "predicted -0.7 | -0.8 true yield\n",
      "\n",
      "COc1ccc(Cl)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.CC(C)c1cc(C(C)C)c(-c2ccccc2P(C2CCCCC2)C2CCCCC2)c(C(C)C)c1.CCN=P(N=P(N(C)C)(N(C)C)N(C)C)(N(C)C)N(C)C.COC(=O)c1cc(-c2cccs2)on1>>COc1ccc(Nc2ccc(C)cc2)cc1\n",
      "predicted -1.2 | -1.2 true yield\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = '../Модели/yieldBERT/rxn_yields/trained_models/buchwald_hartwig/FullCV_01_split_2768/checkpoint-1730-epoch-10'\n",
    "\n",
    "trained_yield_bert = SmilesClassificationModel('bert', model_path,\n",
    "                                               num_labels=1,\n",
    "                                               args={\"regression\": True,\n",
    "                                                     'config': {\"output_hidden_states\": True}},\n",
    "                                               use_cuda=False)\n",
    "\n",
    "yield_predicted = trained_yield_bert.predict(test_df.head(10).text.values)[0]\n",
    "#yield_predicted = yield_predicted * std + mean\n",
    "\n",
    "yield_true = test_df.head(10).labels.values\n",
    "#yield_true = yield_true * std + mean\n",
    "\n",
    "for rxn, pred, true in zip(test_df.head(10).text.values, yield_predicted, yield_true):\n",
    "    print(rxn)\n",
    "    print(f\"predicted {pred:.1f} | {true:.1f} true yield\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9b9b9b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ec4b30e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages/transformers/tokenization_utils.py:831: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c854a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(test_df.iloc[0][\"text\"], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ca15b68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 12,  11,  24,  11,  17,  25,  18,  11,  24,  44,  22,  42,  17,  22,\n",
       "          44,  18,  17,  44,  11,  11,  11,  11,  31,  11,  30,  20,  18,  15,\n",
       "          17,  11,  18,  17,  11,  18,  11,  24,  11,  17,  11,  18,  15,  17,\n",
       "         206,  17,  15,  17,  15,  18,  17,  15,  18,  15,  18,  15,  17,  15,\n",
       "          18,  17,  15,  18,  15,  18,  11,  31,  11,  17,  15,  17,  15,  18,\n",
       "          15,  18,  11,  17,  15,  17,  15,  18,  15,  18,  11,  17,  15,  18,\n",
       "          15,  24,  11,  17,  15,  18,  15,  17,  22,  11,  17,  15,  18,  17,\n",
       "          15,  18,  15,  18,  25,  17,  15,  18,  15,  24,  11,  17,  11,  17,\n",
       "          11,  18,  11,  18,  11,  11,  11,  11,  17,  11,  18,  11,  13]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c0d9fcda",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ClassificationModel object argument after ** must be a mapping, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-bdb428b8184a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: ClassificationModel object argument after ** must be a mapping, not Tensor"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5afbe746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "# Инициализация ClassificationModel\n",
    "model = ClassificationModel(\"bert\",\n",
    "                            \"bert-base-uncased\",\n",
    "                            use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3baf911d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4324fe84c6af47368c8c85fae91fcc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5860dda71ae743f4be98e9da2519b233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5017, -0.7569, -0.8730, -0.7781, -1.1353,  0.4880,  0.5920, -1.1446,\n",
      "        -0.6555, -1.2141])\n"
     ]
    }
   ],
   "source": [
    "results, model_outputs, _ = trained_yield_bert.predict(test_df.head(10).text.values)\n",
    "\n",
    "# Здесь должны бы вывестить эмбеддинги\n",
    "print(torch.tensor(model_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3ebdde2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't find a vocabulary file at path 'smiles'. To load the vocabulary from a Google pretrained model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-1285e939049f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Токенизация строк SMILES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msmiles_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSmilesTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"smiles\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mencoded_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmiles_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages/rxnfp/tokenization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m \u001b[0mto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mSMILES\u001b[0m \u001b[0mcharacter\u001b[0m \u001b[0mper\u001b[0m \u001b[0mline\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \"\"\"\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;31m# take into account special tokens in max length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len_single_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages/transformers/tokenization_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             raise ValueError(\n\u001b[1;32m    188\u001b[0m                 \u001b[0;34m\"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;34m\"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             )\n\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find a vocabulary file at path 'smiles'. To load the vocabulary from a Google pretrained model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`"
     ]
    }
   ],
   "source": [
    "# Пример строки SMILES\n",
    "smiles_inputs = [\"CCO\", \"CCN\"]\n",
    "\n",
    "# Токенизация строк SMILES\n",
    "smiles_tokenizer = SmilesTokenizer(\"smiles\")\n",
    "encoded_inputs = smiles_tokenizer.encode(smiles_inputs, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Инициализация SmilesLanguageModelingModel\n",
    "smiles_lm_model = SmilesLanguageModelingModel(\n",
    "    model_type=\"bert\",\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    train_files=None,  # Для SmilesLanguageModelingModel не требуется обучение\n",
    "    args=None,  # Вы можете передать аргументы по своему усмотрению\n",
    "    use_cuda=False,\n",
    ")\n",
    "\n",
    "# Инференс\n",
    "outputs = smiles_lm_model.model(**encoded_inputs)\n",
    "embeddings = outputs.last_hidden_state.mean(dim=1)  # Предполагается использование среднего пулинга эмбеддингов\n",
    "\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "22b06d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n",
      "Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933b467c85b84b7faecedcc29d2ccf36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c19bbcd054a4a478ac50b9f3a0756f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Загрузка предварительно обученной модели\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = SmilesClassificationModel('bert', model_path, num_labels=1, args={\"regression\": True}, use_cuda=True)\n",
    "\n",
    "# Пример строки SMILES\n",
    "smiles_inputs = [\"CCO\", \"CCN\"]\n",
    "\n",
    "# Инференс\n",
    "result, outputs = model.predict(smiles_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6a2f16b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19940239, 0.30185756], dtype=float32)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7715bd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19940239 0.30185756]\n"
     ]
    }
   ],
   "source": [
    "# Эмбеддинги\n",
    "embeddings = outputs  # предполагается использование среднего пулинга эмбеддингов\n",
    "\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8f5611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "23799f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sliding_window': False,\n",
       " 'tie_value': 1,\n",
       " 'stride': 0.8,\n",
       " 'regression': True,\n",
       " 'lazy_text_column': 0,\n",
       " 'lazy_text_a_column': None,\n",
       " 'lazy_text_b_column': None,\n",
       " 'lazy_labels_column': 1,\n",
       " 'lazy_header_row': True,\n",
       " 'lazy_delimiter': '\\t',\n",
       " 'adam_epsilon': 1e-08,\n",
       " 'best_model_dir': 'outputs/best_model',\n",
       " 'cache_dir': 'cache_dir/',\n",
       " 'config': {'output_hidden_states': True},\n",
       " 'do_lower_case': False,\n",
       " 'early_stopping_consider_epochs': False,\n",
       " 'early_stopping_delta': 0,\n",
       " 'early_stopping_metric': 'eval_loss',\n",
       " 'early_stopping_metric_minimize': True,\n",
       " 'early_stopping_patience': 3,\n",
       " 'encoding': None,\n",
       " 'eval_batch_size': 8,\n",
       " 'evaluate_during_training': True,\n",
       " 'evaluate_during_training_silent': True,\n",
       " 'evaluate_during_training_steps': 2000,\n",
       " 'evaluate_during_training_verbose': False,\n",
       " 'fp16': False,\n",
       " 'fp16_opt_level': 'O1',\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'learning_rate': 9.659e-05,\n",
       " 'local_rank': -1,\n",
       " 'logging_steps': 50,\n",
       " 'manual_seed': 42,\n",
       " 'max_grad_norm': 1.0,\n",
       " 'max_seq_length': 300,\n",
       " 'multiprocessing_chunksize': 500,\n",
       " 'n_gpu': 1,\n",
       " 'no_cache': False,\n",
       " 'no_save': False,\n",
       " 'num_train_epochs': 10,\n",
       " 'output_dir': 'outputs/',\n",
       " 'overwrite_output_dir': True,\n",
       " 'process_count': 158,\n",
       " 'reprocess_input_data': True,\n",
       " 'save_best_model': True,\n",
       " 'save_eval_checkpoints': True,\n",
       " 'save_model_every_epoch': True,\n",
       " 'save_steps': 2000,\n",
       " 'save_optimizer_and_scheduler': True,\n",
       " 'silent': False,\n",
       " 'tensorboard_dir': None,\n",
       " 'train_batch_size': 16,\n",
       " 'use_cached_eval_features': False,\n",
       " 'use_early_stopping': False,\n",
       " 'use_multiprocessing': True,\n",
       " 'wandb_kwargs': {},\n",
       " 'wandb_project': 'buchwald_hartwig_repro_randomsplit',\n",
       " 'warmup_ratio': 0.0,\n",
       " 'warmup_steps': 0,\n",
       " 'weight_decay': 0,\n",
       " 'num_labels': 1,\n",
       " 'model_name': '../Модели/yieldBERT/rxn_yields/trained_models/buchwald_hartwig/FullCV_01_split_2768/checkpoint-1730-epoch-10',\n",
       " 'model_type': 'bert'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_yield_bert.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e89130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d8d42df8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ClassificationArgs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-62f30a547da4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msimpletransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassificationArgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassificationArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"output_hidden_states\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ClassificationArgs'"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationArgs\n",
    "\n",
    "\n",
    "model_args = ClassificationArgs()\n",
    "model_args.config = {\"output_hidden_states\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "94a84716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5017]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.7569]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.8730]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.7781]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.1353]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.4880]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.5920]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.1446]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.6555]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.2141]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    outputs = trained_yield_bert.model(trained_yield_bert.tokenizer.encode(test_df.iloc[i][\"text\"], return_tensors=\"pt\"))\n",
    "    embeddings = outputs[0]\n",
    "    print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fbf77ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.count(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "80704e61",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-549d73f1502d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrained_yield_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_yield_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, text_pair, add_special_tokens, max_length, stride, truncation_strategy, pad_to_max_length, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   1432\u001b[0m             \u001b[0mpad_to_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_to_max_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m             \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1435\u001b[0m         )\n\u001b[1;32m   1436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, max_length, stride, truncation_strategy, pad_to_max_length, is_pretokenized, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, **kwargs)\u001b[0m\n\u001b[1;32m   1574\u001b[0m             )\n\u001b[1;32m   1575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1576\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1577\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m   1554\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m                 raise ValueError(\n\u001b[0;32m-> 1556\u001b[0;31m                     \u001b[0;34m\"Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1557\u001b[0m                 )\n\u001b[1;32m   1558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
     ]
    }
   ],
   "source": [
    "outputs = trained_yield_bert.model(trained_yield_bert.tokenizer.encode(test_df.head(10).text.values, return_tensors=\"pt\"))\n",
    "embeddings = outputs[0]\n",
    "\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "657ddfc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FC(F)(F)c1ccc(Br)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)(F)F.COc1ccc(OC)c(P([C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)[C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)c1-c1c(C(C)C)cc(C(C)C)cc1C(C)C.CCN=P(N=P(N(C)C)(N(C)C)N(C)C)(N(C)C)N(C)C.Fc1cccc(F)c1-c1ccno1>>Cc1ccc(Nc2ccc(C(F)(F)F)cc2)cc1'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[2][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18995c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedf4143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf6b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81992f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359914b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ecabf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deepchem in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (2.5.0)\n",
      "Requirement already satisfied: joblib in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (from deepchem) (1.1.1)\n",
      "Requirement already satisfied: scipy in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (from deepchem) (1.4.1)\n",
      "Requirement already satisfied: numpy in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (from deepchem) (1.19.2)\n",
      "Requirement already satisfied: scikit-learn in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (from deepchem) (0.23.1)\n",
      "Requirement already satisfied: pandas in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (from deepchem) (1.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (from pandas->deepchem) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (from pandas->deepchem) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->deepchem) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (from scikit-learn->deepchem) (3.1.0)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.6.2-cp36-cp36m-manylinux2010_x86_64.whl (458.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 458.3 MB 29 kB/s  eta 0:00:015\n",
      "\u001b[?25hCollecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting keras<2.7,>=2.6.0\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 15.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum~=3.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting gast==0.4.0\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (from tensorflow) (1.19.2)\n",
      "Collecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 21.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.48.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.6 MB 163 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (from tensorflow) (0.37.1)\n",
      "Collecting google-pasta~=0.2\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.7,>=2.6.0\n",
      "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 162 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.7,>=2.6.0\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 17.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse~=1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 162 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (from tensorflow) (4.21.0)\n",
      "Collecting cached-property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "\u001b[K     |████████████████████████████████| 289 kB 16.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 7.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (58.0.4)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "\u001b[K     |████████████████████████████████| 152 kB 23.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.27.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (4.8.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (3.6.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (1.26.18)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: dataclasses in /home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow) (0.8)\n",
      "Building wheels for collected packages: clang, termcolor, wrapt\n",
      "  Building wheel for clang (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30694 sha256=4e428d5545dbd94f131a935c39d8c6e18da5f16b90ff9064928733eeedd83530\n",
      "  Stored in directory: /home/vyacheslav/.cache/pip/wheels/22/4c/94/0583f60c9c5b6024ed64f290cb2d43b06bb4f75577dc3c93a7\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=437a525dd325efff5f6c62ba1f570c86a35bbaafbe14bed08a0bcc095b3766aa\n",
      "  Stored in directory: /home/vyacheslav/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=70104 sha256=d001f60e563adde7610880a481d59ab21f4322d616ccf2f119cc4bc381b5e7ff\n",
      "  Stored in directory: /home/vyacheslav/.cache/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n",
      "Successfully built clang termcolor wrapt\n",
      "Installing collected packages: pyasn1, typing-extensions, six, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, cached-property, absl-py, wrapt, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, clang, astunparse, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.1.1\n",
      "    Uninstalling typing-extensions-4.1.1:\n",
      "      Successfully uninstalled typing-extensions-4.1.1\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "Successfully installed absl-py-0.15.0 astunparse-1.6.3 cached-property-1.5.2 cachetools-4.2.4 clang-5.0 flatbuffers-1.12 gast-0.4.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.48.2 h5py-3.1.0 keras-2.6.0 keras-preprocessing-1.1.2 markdown-3.3.7 oauthlib-3.2.2 opt-einsum-3.3.0 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 six-1.15.0 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.6.2 tensorflow-estimator-2.6.0 termcolor-1.1.0 typing-extensions-3.7.4.3 werkzeug-2.0.3 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install deepchem\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95a73113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from deepchem.feat.smiles_tokenizer import SmilesTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "20e10741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages/transformers/tokenization_utils.py:831: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "tokenizer1 = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b726d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SmilesTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1d1373fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n",
      "Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"
     ]
    }
   ],
   "source": [
    "model = SmilesClassificationModel(\n",
    "    model_type=\"bert\",\n",
    "    model_name=model_path,  # Или другое имя предварительно обученной модели BERT\n",
    "    #tokenizer_type=tokenizer,\n",
    "    #tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME),\n",
    "    use_cuda=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1033b606",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'SmilesClassificationModel' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-3f12eaf52729>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'SmilesClassificationModel' object is not callable"
     ]
    }
   ],
   "source": [
    "outputs = model(tokens)\n",
    "embeddings = outputs[0]\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "cf2f2167",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 12,  11,  11,  11,  24,  11,  17,  25,  18,  11,  24,  44,  22,  42,\n",
       "          17,  22,  44,  18,  17,  44,  11,  11,  11,  11,  31,  11,  30,  20,\n",
       "          18,  15,  17,  11,  18,  17,  11,  18,  11,  24,  11,  17,  11,  18,\n",
       "          15,  17, 206,  17,  15,  17,  15,  18,  17,  15,  18,  15,  18,  15,\n",
       "          17,  15,  18,  17,  15,  18,  15,  18,  11,  31,  11,  17,  15,  17,\n",
       "          15,  18,  15,  18,  11,  17,  15,  17,  15,  18,  15,  18,  11,  17,\n",
       "          15,  18,  15,  24,  11,  17,  15,  18,  15,  17,  22,  11,  17,  15,\n",
       "          18,  17,  15,  18,  15,  18,  25,  17,  15,  18,  15,  24,  11,  17,\n",
       "          11,  17,  11,  18,  11,  18,  11,  11,  11,  11,  17,  11,  18,  11,\n",
       "          11,  11,  13]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9a007884",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Размораживаем эмбеддинги\n",
    "for param in model.model.bert.embeddings.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "08249949",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Токенизация SMILES строки\n",
    "tokens = tokenizer1.encode_plus(str(test_df.head(2).text.values),\n",
    "                           return_tensors=\"pt\")\n",
    "\n",
    "# Получение эмбеддингов\n",
    "with torch.no_grad():\n",
    "    outputs = model.model(tokens['input_ids'])\n",
    "    embeddings = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "71b5b2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03312767]], dtype=float32)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b1644907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0331]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36388513",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in frozen_model.model.classifier.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in frozen_model.model.bert.pooler.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in frozen_model.model.bert.encoder.layer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in frozen_model.model.bert.embeddings.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "032946cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0111]]),)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "23838f27",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertModel' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-f6c445db11aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 594\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "89e6b1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(591, 256, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 256)\n",
      "      (token_type_embeddings): Embedding(2, 256)\n",
      "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.7987, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.7987, inplace=False)\n",
      "  (classifier): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.7987,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 591\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.model)\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "05af35a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(591, 256, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 256)\n",
       "      (token_type_embeddings): Embedding(2, 256)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.7987, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.7987, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.7987, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.7987, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.7987, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.7987, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.7987, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.7987, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.7987, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.7987, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.7987, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.7987, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.7987, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.7987, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.7987, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.7987, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.7987, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.7987, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.7987, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.7987, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.7987, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.7987, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.7987, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.7987, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.7987, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.7987, inplace=False)\n",
       "  (classifier): Linear(in_features=256, out_features=1, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02a86df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a75fa340",
   "metadata": {},
   "source": [
    "## Получение эмбеддингов с классической модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bcfd3cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Инициализация токенизатора и модели\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = SmilesTokenizer\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Пример SMILES строки\n",
    "smiles_string = \"CC(=O)OC1=CC=CC=C1C(=O)O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c0229923",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "encode() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-2becf6a7eaf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Токенизация SMILES строки\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m tokens = tokenizer.encode(str(test_df.head(1).text.values),\n\u001b[0;32m----> 3\u001b[0;31m                           return_tensors=\"pt\")\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Получение эмбеддингов\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: encode() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "# Токенизация SMILES строки\n",
    "tokens = tokenizer.encode(str(test_df.head(1).text.values),\n",
    "                          return_tensors=\"pt\")\n",
    "\n",
    "# Получение эмбеддингов\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens)\n",
    "    embeddings = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "db73da3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1031,  1005,  7987,  2278,  2487,  9468,  9468,  2078,  2487,\n",
       "          1012, 10507,  2487,  9468,  2278,  1006,  1050,  1007, 10507,  2487,\n",
       "          1012,  1051,  1027,  1055,  1006,  1027,  1051,  1007,  1006,  1051,\n",
       "          1031, 22851,  1033,  1015,  2278,  2475,  9468,  9468,  2278,  2475,\n",
       "          1011, 29248,  9468,  9468,  2278,  2475,  2078,  1066,  1015,  1007,\n",
       "          1039,  1006,  1042,  1007,  1006,  1042,  1007,  1042,  1012,  2522,\n",
       "          2278,  2487,  9468,  2278,  1006,  1051,  2278,  1007,  1039,  1006,\n",
       "          1052,  1006,  1039,  1006,  1039,  1007,  1006,  1039,  1007,  1039,\n",
       "          1007,  1039,  1006,  1039,  1007,  1006,  1039,  1007,  1039,  1007,\n",
       "         27723,  1011, 27723,  2278,  1006,  1039,  1006,  1039,  1007,  1039,\n",
       "          1007, 10507,  1006,  1039,  1006,  1039,  1007,  1039,  1007, 10507,\n",
       "          2487,  2278,  1006,  1039,  1007,  1039,  1012, 27166,  1006,  1039,\n",
       "          1007,  1039,  1006,  1027, 13316,  1006,  1039,  1007,  1006,  1039,\n",
       "          1007,  1039,  1007,  1050,  1006,  1039,  1007,  1039,  1012, 27723,\n",
       "          9468,  2278,  1006, 27166,  1006, 10507,  2475,  9468,  9468,  2278,\n",
       "          2475,  1007, 29248,  9468,  3630,  2475,  1007, 10507,  2487,  1028,\n",
       "          1028, 10507,  2487,  9468,  2278,  1006, 13316,  2475,  9468,  9468,\n",
       "          2078,  2475,  1007, 10507,  2487,  1005,  1033,   102]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c43f29d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4294, -0.0930, -0.2599,  ..., -0.3932,  0.6979,  0.8974],\n",
       "         [ 0.3636,  0.5580,  0.0487,  ...,  0.5626,  0.3386,  0.0577],\n",
       "         [ 0.5226,  0.3864,  0.9493,  ...,  0.1434,  0.1315,  0.0573],\n",
       "         ...,\n",
       "         [ 0.7180,  0.4761,  1.1916,  ...,  0.1442,  0.1879, -0.4327],\n",
       "         [ 0.3096,  0.1615,  0.9234,  ...,  0.2996,  0.2408, -0.0898],\n",
       "         [ 0.1719, -0.1700,  0.4720,  ...,  0.4140, -0.2088,  0.4093]]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81078cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "406d13fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rxnfp.tokenization import SmilesTokenizer\n",
    "from transformers import BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1344208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages/transformers/tokenization_utils.py:831: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "tokenizer1 = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "070bbfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = tokenizer1.encode_plus(text=str(test_df.head(1).labels.values),\n",
    "                                    max_length=trained_yield_bert.config.max_position_embeddings,\n",
    "                                    padding=True,\n",
    "                                    truncation=True,\n",
    "                                    return_tensors='pt')\n",
    "                                    #self=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "65fa25fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = trained_yield_bert.model(**bert_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8f6235d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-64b3052bbe85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_hidden_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "embeddings = output['last_hidden_state'].squeeze()[0].cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c37a1048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4775]]),)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2ffd06fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(591, 256, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 256)\n",
      "      (token_type_embeddings): Embedding(2, 256)\n",
      "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.7987, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.7987, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.7987, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.7987, inplace=False)\n",
      "  (classifier): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(trained_yield_bert.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "26a26416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sliding_window': False,\n",
       " 'tie_value': 1,\n",
       " 'stride': 0.8,\n",
       " 'regression': True,\n",
       " 'lazy_text_column': 0,\n",
       " 'lazy_text_a_column': None,\n",
       " 'lazy_text_b_column': None,\n",
       " 'lazy_labels_column': 1,\n",
       " 'lazy_header_row': True,\n",
       " 'lazy_delimiter': '\\t',\n",
       " 'adam_epsilon': 1e-08,\n",
       " 'best_model_dir': 'outputs/best_model',\n",
       " 'cache_dir': 'cache_dir/',\n",
       " 'config': {'output_hidden_states': True},\n",
       " 'do_lower_case': False,\n",
       " 'early_stopping_consider_epochs': False,\n",
       " 'early_stopping_delta': 0,\n",
       " 'early_stopping_metric': 'eval_loss',\n",
       " 'early_stopping_metric_minimize': True,\n",
       " 'early_stopping_patience': 3,\n",
       " 'encoding': None,\n",
       " 'eval_batch_size': 8,\n",
       " 'evaluate_during_training': True,\n",
       " 'evaluate_during_training_silent': True,\n",
       " 'evaluate_during_training_steps': 2000,\n",
       " 'evaluate_during_training_verbose': False,\n",
       " 'fp16': False,\n",
       " 'fp16_opt_level': 'O1',\n",
       " 'gradient_accumulation_steps': 1,\n",
       " 'learning_rate': 9.659e-05,\n",
       " 'local_rank': -1,\n",
       " 'logging_steps': 50,\n",
       " 'manual_seed': 42,\n",
       " 'max_grad_norm': 1.0,\n",
       " 'max_seq_length': 300,\n",
       " 'multiprocessing_chunksize': 500,\n",
       " 'n_gpu': 1,\n",
       " 'no_cache': False,\n",
       " 'no_save': False,\n",
       " 'num_train_epochs': 10,\n",
       " 'output_dir': 'outputs/',\n",
       " 'overwrite_output_dir': True,\n",
       " 'process_count': 158,\n",
       " 'reprocess_input_data': True,\n",
       " 'save_best_model': True,\n",
       " 'save_eval_checkpoints': True,\n",
       " 'save_model_every_epoch': True,\n",
       " 'save_steps': 2000,\n",
       " 'save_optimizer_and_scheduler': True,\n",
       " 'silent': False,\n",
       " 'tensorboard_dir': None,\n",
       " 'train_batch_size': 16,\n",
       " 'use_cached_eval_features': False,\n",
       " 'use_early_stopping': False,\n",
       " 'use_multiprocessing': True,\n",
       " 'wandb_kwargs': {},\n",
       " 'wandb_project': 'buchwald_hartwig_repro_randomsplit',\n",
       " 'warmup_ratio': 0.0,\n",
       " 'warmup_steps': 0,\n",
       " 'weight_decay': 0,\n",
       " 'num_labels': 1,\n",
       " 'model_name': '../Модели/yieldBERT/rxn_yields/trained_models/buchwald_hartwig/FullCV_01_split_2768/checkpoint-1730-epoch-10',\n",
       " 'model_type': 'bert'}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_yield_bert.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d1fa713c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_yield_bert.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362c256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a898b936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4b1c6ae",
   "metadata": {},
   "source": [
    "### Work version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36d6c595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from rxnfp.models import SmilesClassificationModel, SmilesLanguageModelingModel\n",
    "import pandas as pd\n",
    "from rxn_yields.data import generate_buchwald_hartwig_rxns\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import DataStructs, AllChem\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b093c2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clc1ccccn1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2...</td>\n",
       "      <td>1.387974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brc1ccccn1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2...</td>\n",
       "      <td>-0.796876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCc1ccc(I)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccc...</td>\n",
       "      <td>-0.827835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FC(F)(F)c1ccc(Cl)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd...</td>\n",
       "      <td>-0.464841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COc1ccc(Cl)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2cc...</td>\n",
       "      <td>-1.186082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    labels\n",
       "0  Clc1ccccn1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2...  1.387974\n",
       "1  Brc1ccccn1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2... -0.796876\n",
       "2  CCc1ccc(I)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccc... -0.827835\n",
       "3  FC(F)(F)c1ccc(Cl)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd... -0.464841\n",
       "4  COc1ccc(Cl)cc1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2cc... -1.186082"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('../Модели/yieldBERT/rxn_yields/data/Buchwald-Hartwig/Dreher_and_Doyle_input_data.xlsx', sheet_name='FullCV_01')\n",
    "df['rxn'] = generate_buchwald_hartwig_rxns(df)\n",
    "\n",
    "train_df = df.iloc[:2767][['rxn', 'Output']] \n",
    "test_df = df.iloc[2767:][['rxn', 'Output']] #\n",
    "\n",
    "train_df.columns = ['text', 'labels']\n",
    "test_df.columns = ['text', 'labels']\n",
    "mean = train_df.labels.mean()\n",
    "std = train_df.labels.std()\n",
    "train_df['labels'] = (train_df['labels'] - mean) / std\n",
    "test_df['labels'] = (test_df['labels'] - mean) / std\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "224b197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../Модели/yieldBERT/rxn_yields/trained_models/buchwald_hartwig/FullCV_01_split_2768/checkpoint-1730-epoch-10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c2f8a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n",
      "Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"
     ]
    }
   ],
   "source": [
    "trained_yield_bert = SmilesClassificationModel('bert', model_path,\n",
    "                                               num_labels=1,\n",
    "                                               args={\"regression\": True,\n",
    "                                                    'config': {\"output_hidden_states\": True}},\n",
    "                                               use_cuda=False,\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f252c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vyacheslav/anaconda/anaconda3/envs/yields/lib/python3.6/site-packages/transformers/tokenization_utils.py:831: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "tokenizer1 = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77de108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = tokenizer1.batch_encode_plus(str(test_df.head(1).labels.values),\n",
    "                                           max_length=trained_yield_bert.config.max_position_embeddings,\n",
    "                                           padding=True,\n",
    "                                           truncation=True,\n",
    "                                           pad_to_max_length=True,\n",
    "                                           return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b0c113a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "625e6456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[12, 11, 13,  ...,  0,  0,  0],\n",
       "        [12, 11, 13,  ...,  0,  0,  0],\n",
       "        [12, 24, 13,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [12, 43, 13,  ...,  0,  0,  0],\n",
       "        [12, 98, 13,  ...,  0,  0,  0],\n",
       "        [12, 11, 13,  ...,  0,  0,  0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c302869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = trained_yield_bert.model(**bert_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0476efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b942b734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.672431230545044,\n",
       " 0.672431230545044,\n",
       " 0.8746748566627502,\n",
       " 0.6140751242637634,\n",
       " 0.5577840805053711,\n",
       " 0.522050142288208,\n",
       " 0.6576945781707764,\n",
       " 0.6140751242637634,\n",
       " 0.5635161995887756,\n",
       " 0.5149366855621338,\n",
       " 0.5635161995887756,\n",
       " 0.672431230545044]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [CLS] token embeddings in position 0\n",
    "embeddings = output[0].squeeze().cpu().numpy().tolist()\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31834b93",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-3387ee446f78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_hidden_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "output['last_hidden_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abda3666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98bc9d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c5331ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[1][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88c46d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[1][1][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c0d8f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "cc615c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54bafe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937c887e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
